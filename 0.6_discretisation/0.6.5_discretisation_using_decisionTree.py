""" 
Discretisation with Decision Trees
- Discretisation with Decision Trees consists in using a decision tree to identify the optimal bins. 
- When a decision tree makes a decision, it assigns an observation to one of n end leaves. 
- Therefore, any decision tree will generate a discrete output, which values are the predictions at each of its n leaves.

How to do discretisation with trees?
    1) Train a decision tree of limited depth (2, 3 or 4) using the variable we want to discretise and the target.
    2) Replace the values by the output returned by the tree.

Advantages
- The output returned by the decision tree is monotonically related to the target.
- The tree end nodes, or bins in the discretised variable show decreased entropy: that is, the observations within 
        each bin are more similar among themselves than to those of other bins.

Limitations
- Prone over-fitting
- More importantly, some tuning of the tree parameters needed to obtain the optimal number of splits 
        (e.g., tree depth, minimum number of samples in one partition, maximum number of partitions, and a minimum 
        information gain). This it can be time consuming.

In this example
- We will learn how to perform discretisation with decision trees using the Titanic dataset.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, export_graphviz
from sklearn.model_selection import cross_val_score

# load the numerical variables of the Titanic Dataset
data = pd.read_csv('dataset/titanic.csv',usecols = ['age', 'fare', 'survived'])


data.head()
"""    
    survived      age      fare
0         1  29.0000  211.3375
1         1   0.9167  151.5500
2         0   2.0000  151.5500
3         0  30.0000  151.5500
4         0  25.0000  151.5500 """

# Let's separate into train and test set
X_train, X_test, y_train, y_test = train_test_split(
    data[['age', 'fare']],
    data['survived'],
    test_size=0.3,
    random_state=0)

X_train.shape, X_test.shape
# ((916, 2), (393, 2))

# The variables Age and Fare contain missing data, that I will fill by extracting a random sample of the variable.

def impute_na(data, variable):
    df = data.copy()
    
    # random sampling
    df[variable+'_random'] = df[variable]
    # extract the random sample to fill the na
    random_sample = X_train[variable].dropna().sample(df[variable].isnull().sum(), random_state=0)
    # pandas needs to have the same index in order to merge datasets
    random_sample.index = df[df[variable].isnull()].index
    df.loc[df[variable].isnull(), variable+'_random'] = random_sample
    
    return df[variable+'_random']
# replace NA in both  train and test sets

X_train['age'] = impute_na(data, 'age')
X_test['age'] = impute_na(data, 'age')

X_train['fare'] = impute_na(data, 'fare')
X_test['fare'] = impute_na(data, 'fare')

X_train.head()
"""
        age     fare
501   13.0  19.5000
588    4.0  23.0000
402   30.0  13.8583
1193  21.0   7.7250
686   22.0   7.7250 """

# example: build Classification tree using Age to predict Survived

tree_model = DecisionTreeClassifier(max_depth=3)

tree_model.fit(X_train['age'].to_frame(), y_train)

X_train['Age_tree'] = tree_model.predict_proba(X_train['age'].to_frame())[:,1]

X_train.head(10)
""" 
       age      fare  Age_tree
501   13.0   19.5000  0.366059
588    4.0   23.0000  0.529412
402   30.0   13.8583  0.366059
1193  21.0    7.7250  0.366059
686   22.0    7.7250  0.366059
971   16.0    7.8792  0.366059
117   30.0   56.9292  0.366059
540    2.0   26.0000  0.529412
294   49.0  110.8833  0.450704
261   35.0   26.2875  0.366059 """

# let's explore how many end points the tree created

X_train['Age_tree'].unique()
# array([0.36605891, 0.52941176, 0.45070423, 0.        , 1.        ])

# A tree of depth 2, makes 2 splits, therefore generating 4 buckets, that is why we see 4 different probabilities 
# in the output above.

# monotonic relationship with target

pd.concat([X_train, y_train], axis=1).groupby(['Age_tree'])['survived'].mean().plot()
plt.title('Monotonic relationship between discretised Age and target')
plt.ylabel('Survived')
plt.show()

# number of passengers per probabilistic bucket / bin

X_train.groupby(['Age_tree'])['age'].count().plot.bar()
plt.show()

# median age within each bucket originated by the tree

X_train.groupby(['Age_tree'])['age'].median().plot.bar()
plt.show()

# let's see the Age limits buckets generated by the tree
# by capturing the minimum and maximum age per each probability bucket, 
# we get an idea of the bucket cut-offs

pd.concat( [X_train.groupby(['Age_tree'])['age'].min(),
            X_train.groupby(['Age_tree'])['age'].max()], axis=1)
"""
age      age        age_tree
0.000000  65.0000  74.0000
0.366059   9.0000  44.0000
0.450704  45.0000  64.0000
0.529412   0.7500   8.0000
1.000000   0.1667   0.1667 

- Thus, the decision tree generated the buckets: 65-74, 9-44, 45-64 and 0.7-8 and 0-16-0.16, 
        with probabilities of survival of .0, .36, .45, .52 and .1 respectively.

Tree visualisation"""

# we can go ahead and visualise the tree by saving the model to a file,
# and opening that file in the below indicated link

with open("0.6_discretisation/tree_model.txt", "w") as f:
    f = export_graphviz(tree_model, out_file=f)

# go here to open the file: http://webgraphviz.com
# this is what you should see if you do what is described in the previous cell

# I saved the image you should retrieve in the server above into a png, and then load
# it here to smooth the demo

# the  plot indicates  the age cut-offs at each node, and also the number of samples at each node, and 
# the gini

from IPython.display import Image
from IPython.core.display import HTML 
PATH = "0.6_discretisation/tree_visualisation.png"
Image(filename = PATH , width=1000, height=1000)

# Let's expand the tree results to the test set, and explore the monotonic relationship

X_test['Age_tree'] = tree_model.predict_proba(X_test['age'].to_frame())[:,1]

# monotonic relationship with target

pd.concat([X_test, y_test], axis=1).groupby(['Age_tree'])['survived'].mean().plot()
plt.title('Monotonic relationship between discretised Age and target')
plt.ylabel('Survived')
plt.show()

""" 
- We can see that the monotonic relationship is not maintained in the test set, 
        which probably indicates that the tree we build was over-fitting to the train set.

Building the optimal decision tree
- There are a number of parameters that we could optimise to obtain the best bin split using decision trees.

- I will optimise the tree depth for this example. 
- But remember that we could also optimise the remaining parameters of the decision tree.

Visit sklearn website to see which other parameters can be optimised. """

# Build trees of different depths, and calculate the roc-auc of each tree
# choose the depth that generates the best roc-auc

score_ls = []  # here we store the roc auc
score_std_ls = []  # here we store the standard deviation of the roc_auc

for tree_depth in [1, 2, 3, 4]:

    # call the model
    tree_model = DecisionTreeClassifier(max_depth=tree_depth)

    # train the model using 3 fold cross validation

    scores = cross_val_score(
        tree_model, X_train['age'].to_frame(), y_train, cv=3, scoring='roc_auc')
    
    # save the parameters
    score_ls.append(np.mean(scores))
    score_std_ls.append(np.std(scores))

    
# capture the parameters in a dataframe
temp = pd.concat([pd.Series([1, 2, 3, 4]), pd.Series(
    score_ls), pd.Series(score_std_ls)], axis=1)

temp.columns = ['depth', 'roc_auc_mean', 'roc_auc_std']
temp
""" 
   depth  roc_auc_mean  roc_auc_std
0      1      0.501610     0.010728
1      2      0.526407     0.018134
2      3      0.515982     0.029126
3      4      0.527269     0.024281 """

# We obtain the best roc-auc using depths of 2 (same value as depth 4 but smaller std).
#  I will select depth of 2 to proceed.

# Transform the feature using tree
tree_model = DecisionTreeClassifier(max_depth=2)

tree_model.fit(X_train['age'].to_frame(), y_train)

X_train['Age_tree'] = tree_model.predict_proba(X_train['age'].to_frame())[:, 1]
X_test['Age_tree'] = tree_model.predict_proba(X_test['age'].to_frame())[:, 1]
# monotonic relationship with target in train set

pd.concat([X_train, y_train], axis=1).groupby(['Age_tree'])['survived'].mean().plot()
plt.title('Monotonic relationship between discretised Age and target')
plt.ylabel('Survived')
plt.show()

# and in the test set

X_test['Age_tree'] = tree_model.predict_proba(X_test['age'].to_frame())[:,1]

# monotonic relationship with target
pd.concat([X_test, y_test], axis=1).groupby(['Age_tree'])['survived'].mean().plot()
plt.title('Monotonic relationship between discretised Age and target')
plt.ylabel('Survived')
plt.show()

# Now the monotonic relationship is maintained in the test set as well.

# That is all for this EXAMPLE. I hope you enjoyed the information, and see you in the next one.